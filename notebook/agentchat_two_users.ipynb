{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/microsoft/autogen/blob/main/notebook/agentchat_two_users.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Auto Generated Agent Chat: Collaborative Task Solving with Multiple Agents and Human Users\n",
    "\n",
    "AutoGen offers conversable agents powered by LLM, tool or human, which can be used to perform tasks collectively via automated chat. This framework allows tool use and human participation through multi-agent conversation. Please find documentation about this feature [here](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat).\n",
    "\n",
    "In this notebook, we demonstrate an application involving multiple agents and human users to work together and accomplish a task. `AssistantAgent` is an LLM-based agent that can write Python code (in a Python coding block) for a user to execute for a given task. `UserProxyAgent` is an agent which serves as a proxy for a user to execute the code written by `AssistantAgent`. We create multiple `UserProxyAgent` instances which can represent different human users.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "AutoGen requires `Python>=3.8`. To run this notebook example, please install:\n",
    "```bash\n",
    "pip install pyautogen\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T23:40:52.317406Z",
     "iopub.status.busy": "2023-02-13T23:40:52.316561Z",
     "iopub.status.idle": "2023-02-13T23:40:52.321193Z",
     "shell.execute_reply": "2023-02-13T23:40:52.320628Z"
    }
   },
   "outputs": [],
   "source": [
    "# %pip install pyautogen~=0.1.1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set your API Endpoint\n",
    "\n",
    "The [`config_list_from_json`](https://microsoft.github.io/autogen/docs/reference/oai/openai_utils#config_list_from_json) function loads a list of configurations from an environment variable or a json file.\n",
    "\n",
    "It first looks for an environment variable of a specified name (\"OAI_CONFIG_LIST\" in this example) which needs to be a valid json string. If that variable is not found, it then looks for a json file with the same name. It filters the configs by models (you can filter by other keys as well).\n",
    "\n",
    "The json looks like the following:\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"model\": \"gpt-4\",\n",
    "        \"api_key\": \"<your OpenAI API key here>\"\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"gpt-4\",\n",
    "        \"api_key\": \"<your Azure OpenAI API key here>\",\n",
    "        \"api_base\": \"<your Azure OpenAI API base here>\",\n",
    "        \"api_type\": \"azure\",\n",
    "        \"api_version\": \"2023-07-01-preview\"\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"gpt-4-32k\",\n",
    "        \"api_key\": \"<your Azure OpenAI API key here>\",\n",
    "        \"api_base\": \"<your Azure OpenAI API base here>\",\n",
    "        \"api_type\": \"azure\",\n",
    "        \"api_version\": \"2023-07-01-preview\"\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "If you open this notebook in colab, you can upload your files by clicking the file icon on the left panel and then choose \"upload file\" icon.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'model': 'gpt-4', 'api_key': '5d050ffec2b94f5eb43c54c80149561e', 'api_base': 'https://futu-002-caeast-001.openai.azure.com/', 'api_type': 'azure', 'api_version': '2023-07-01-preview'}, {'model': 'gpt-4-32k', 'api_key': '5d050ffec2b94f5eb43c54c80149561e', 'api_base': 'https://futu-002-caeast-001.openai.azure.com/', 'api_type': 'azure', 'api_version': '2023-07-01-preview'}]\n"
     ]
    }
   ],
   "source": [
    "import autogen\n",
    "\n",
    "config_list = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-4\", \"gpt4\", \"gpt-4-32k\", \"gpt-4-32k-0314\", \"gpt-4-32k-v0314\"],\n",
    "    },\n",
    ")\n",
    "print(config_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trail_id = 46\n",
    "default_llm_config = {\n",
    "        \"request_timeout\": 600,\n",
    "        \"seed\": trail_id,\n",
    "        # Excluding azure openai endpoints from the config list.\n",
    "        # Change to `exclude=\"openai\"` to exclude openai endpoints, or remove the `exclude` argument to include both.\n",
    "        \"config_list\": config_list,\n",
    "        \"model\": \"gpt-4\",  # make sure the endpoint you use supports the model\n",
    "        \"temperature\": 0\n",
    "    }\n",
    "                      "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Agents\n",
    "\n",
    "We define `ask_expert` function to start a conversation between two agents and return a summary of the result. We construct an assistant agent named \"assistant_for_expert\" and a user proxy agent named \"expert\". We specify `human_input_mode` as \"ALWAYS\" in the user proxy agent, which will always ask for feedback from the expert user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_expert(message):\n",
    "    \"\"\"\n",
    "    将一个任务(message)询问专家(ask_expert)\n",
    "    \"\"\"\n",
    "    # 专家的助理(专家自己不干活，都是他的助理干活，他只发号施令，叫专家的助理干活)\n",
    "    assistant_for_expert = autogen.AssistantAgent(\n",
    "        name=\"assistant_for_expert\",\n",
    "        max_consecutive_auto_reply=10,\n",
    "        code_execution_config={\"work_dir\": \"assistant_for_expert\"},\n",
    "        llm_config=default_llm_config\n",
    "    )\n",
    "    # 专家本家，不干活只发命令\n",
    "    expert = autogen.UserProxyAgent(\n",
    "        name=\"expert\",\n",
    "        human_input_mode=\"ALWAYS\",\n",
    "        max_consecutive_auto_reply=10,\n",
    "        code_execution_config={\"work_dir\": \"expert\"},\n",
    "    )\n",
    "\n",
    "    # 专家接收到了任务，将这个任务发送给他的助理\n",
    "    expert.initiate_chat(assistant_for_expert, message=message)\n",
    "    # TODO: stop_reply_at_receive没懂\n",
    "    expert.stop_reply_at_receive(assistant_for_expert)\n",
    "    # expert.human_input_mode, expert.max_consecutive_auto_reply = \"NEVER\", 0\n",
    "    # final message sent from the expert\n",
    "    # 整个问题解决完之后(可能涉及来回多步对话)，专家发出最后一个命令: 让助理将以上解决方案总结一下，并以一种容易理解的方式解释答案\n",
    "    expert.send(\"summarize the solution and explain the answer in an easy-to-understand way\", assistant_for_expert)\n",
    "    # return the last message the expert received\n",
    "    # 助理完成总结后，发送最后一条消息给专家，这个消息作为整个ask_expert(message)的返回结果\n",
    "    return expert.last_message()[\"content\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We construct another assistant agent named \"assistant_for_student\" and a user proxy agent named \"student\". We specify `human_input_mode` as \"TERMINATE\" in the user proxy agent, which will ask for feedback when it receives a \"TERMINATE\" signal from the assistant agent. We set the `functions` in `AssistantAgent` and `function_map` in `UserProxyAgent` to use the created `ask_expert` function.\n",
    "\n",
    "For simplicity, the `ask_expert` function is defined to run locally. For real applications, the function should run remotely to interact with an expert user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "刚才有:\n",
    "1. 专家\n",
    "2. 专家的助理\n",
    "\n",
    "现在有:\n",
    "3. 学生\n",
    "4. 学生的助理\n",
    "    - system_message不再使用DEFAULT_SYSTEM_MESSAGE, 而是改成更简短的\n",
    "'''\n",
    "llm_config_with_func = default_llm_config.copy()\n",
    "llm_config_with_func[\"functions\"] = [\n",
    "            {\n",
    "                \"name\": \"ask_expert\",\n",
    "                \"description\": \"ask expert when you can't solve the problem satisfactorily.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"message\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"question to ask expert. Make sure the question include enough context, such as the code and the execution result. The expert does not know the conversation between you and the user, unless you share the conversation with the expert.\",\n",
    "                        },\n",
    "                    },\n",
    "                    \"required\": [\"message\"],\n",
    "                },\n",
    "            },\n",
    "        ]\n",
    "\n",
    "# 学生的助理有一个工具: ask_expert\n",
    "assistant_for_student = autogen.AssistantAgent(\n",
    "    name=\"assistant_for_student\",\n",
    "    system_message=\"You are a helpful assistant. Reply TERMINATE when the task is done.\",\n",
    "    llm_config=llm_config_with_func\n",
    ")\n",
    "\n",
    "# 学生提供给助理一个工具: 问专家\n",
    "student = autogen.UserProxyAgent(\n",
    "    name=\"student\",\n",
    "    human_input_mode=\"TERMINATE\",\n",
    "    max_consecutive_auto_reply=10,\n",
    "    code_execution_config={\"work_dir\": \"student\"},\n",
    "    function_map={\"ask_expert\": ask_expert},\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a task\n",
    "\n",
    "We invoke the `initiate_chat()` method of the student proxy agent to start the conversation. When you run the cell below, you will be prompted to provide feedback after the assistant agent sends a \"TERMINATE\" signal in the end of the message. If you don't provide any feedback (by pressing Enter directly), the conversation will finish. Before the \"TERMINATE\" signal, the student proxy agent will try to execute the code suggested by the assistant agent on behalf of the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mstudent\u001b[0m (to assistant_for_student):\n",
      "\n",
      "Find $a + b + c$, given that $x+y \\neq -1$ and \n",
      "\\begin{align}\n",
      "\tax + by + c & = x + 7,\\\n",
      "\ta + bx + cy & = 2x + 6y,\\\n",
      "\tay + b + cx & = 4x + y.\n",
      "\\end{align}.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant_for_student\u001b[0m (to student):\n",
      "\n",
      "To find the values of $a$, $b$, and $c$, we can solve the system of equations. However, the system of equations you provided is not in the standard form. The standard form of a system of linear equations is $ax + by = c$. \n",
      "\n",
      "Let's rewrite the equations in the standard form:\n",
      "\n",
      "1. $ax + by - x = 7 - c$\n",
      "2. $a + bx - 2x = 6y - cy$\n",
      "3. $ay + b - y = 4x - cx$\n",
      "\n",
      "Now, we can solve this system of equations. However, it seems that there might be a mistake in the equations you provided. The system of equations is not consistent, and it seems impossible to solve for $a$, $b$, and $c$ using these equations. \n",
      "\n",
      "Could you please check the equations again?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mstudent\u001b[0m (to assistant_for_student):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant_for_student\u001b[0m (to student):\n",
      "\n",
      "\u001b[32m***** Suggested function Call: ask_expert *****\u001b[0m\n",
      "Arguments: \n",
      "{\n",
      "  \"message\": \"The user provided a system of equations and asked to find $a + b + c$. The equations are:\\n\\n1. $ax + by + c = x + 7$\\n2. $a + bx + cy = 2x + 6y$\\n3. $ay + b + cx = 4x + y$\\n\\nHowever, I'm not sure how to solve this system of equations. Could you please provide some guidance?\"\n",
      "}\n",
      "\u001b[32m***********************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION ask_expert...\u001b[0m\n",
      "\u001b[33mexpert\u001b[0m (to assistant_for_expert):\n",
      "\n",
      "The user provided a system of equations and asked to find $a + b + c$. The equations are:\n",
      "\n",
      "1. $ax + by + c = x + 7$\n",
      "2. $a + bx + cy = 2x + 6y$\n",
      "3. $ay + b + cx = 4x + y$\n",
      "\n",
      "However, I'm not sure how to solve this system of equations. Could you please provide some guidance?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant_for_expert\u001b[0m (to expert):\n",
      "\n",
      "Sure, I can help with that. We can solve this system of equations using Python's sympy library, which is a Python library for symbolic mathematics. \n",
      "\n",
      "Here is the plan:\n",
      "1. We will define the symbols for the variables we need to solve for: a, b, c, x, y.\n",
      "2. We will define the equations using these symbols.\n",
      "3. We will use sympy's solve function to solve the system of equations.\n",
      "4. We will add the solutions for a, b, and c to find the value of $a + b + c$.\n",
      "\n",
      "Let's execute this plan.\n",
      "\n",
      "```python\n",
      "# python code\n",
      "from sympy import symbols, Eq, solve\n",
      "\n",
      "# Step 1: Define the symbols\n",
      "a, b, c, x, y = symbols('a b c x y')\n",
      "\n",
      "# Step 2: Define the equations\n",
      "eq1 = Eq(a*x + b*y + c, x + 7)\n",
      "eq2 = Eq(a + b*x + c*y, 2*x + 6*y)\n",
      "eq3 = Eq(a*y + b + c*x, 4*x + y)\n",
      "\n",
      "# Step 3: Solve the system of equations\n",
      "solution = solve((eq1, eq2, eq3), (a, b, c))\n",
      "\n",
      "# Step 4: Add the solutions for a, b, and c\n",
      "sum_abc = solution[a] + solution[b] + solution[c]\n",
      "\n",
      "print(f\"The value of a + b + c is: {sum_abc}\")\n",
      "```\n",
      "\n",
      "Please run this code to find the value of $a + b + c$.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/jarvixwang/Documents/Project/autogen/notebook/agentchat_two_users.ipynb 单元格 12\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jarvixwang/Documents/Project/autogen/notebook/agentchat_two_users.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# the assistant receives a message from the student, which contains the task description\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jarvixwang/Documents/Project/autogen/notebook/agentchat_two_users.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m student\u001b[39m.\u001b[39;49minitiate_chat(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jarvixwang/Documents/Project/autogen/notebook/agentchat_two_users.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     assistant_for_student,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jarvixwang/Documents/Project/autogen/notebook/agentchat_two_users.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     message\u001b[39m=\u001b[39;49m\u001b[39m\"\"\"\u001b[39;49m\u001b[39mFind $a + b + c$, given that $x+y \u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mneq -1$ and \u001b[39;49m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jarvixwang/Documents/Project/autogen/notebook/agentchat_two_users.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m\\\\\u001b[39;49;00m\u001b[39mbegin\u001b[39;49m\u001b[39m{align}\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jarvixwang/Documents/Project/autogen/notebook/agentchat_two_users.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m\tax + by + c & = x + 7,\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jarvixwang/Documents/Project/autogen/notebook/agentchat_two_users.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m\ta + bx + cy & = 2x + 6y,\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jarvixwang/Documents/Project/autogen/notebook/agentchat_two_users.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m\tay + b + cx & = 4x + y.\u001b[39;49m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jarvixwang/Documents/Project/autogen/notebook/agentchat_two_users.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m\\\\\u001b[39;49;00m\u001b[39mend\u001b[39;49m\u001b[39m{align}\u001b[39;49;00m\u001b[39m.\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jarvixwang/Documents/Project/autogen/notebook/agentchat_two_users.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jarvixwang/Documents/Project/autogen/notebook/agentchat_two_users.ipynb#X13sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:531\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[0;34m(self, recipient, clear_history, silent, **context)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Initiate a chat with the recipient agent.\u001b[39;00m\n\u001b[1;32m    518\u001b[0m \n\u001b[1;32m    519\u001b[0m \u001b[39mReset the consecutive auto reply counter.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[39m        \"message\" needs to be provided if the `generate_init_message` method is not overridden.\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    530\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_chat(recipient, clear_history)\n\u001b[0;32m--> 531\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_init_message(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcontext), recipient, silent\u001b[39m=\u001b[39;49msilent)\n",
      "File \u001b[0;32m~/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:334\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    332\u001b[0m valid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_append_oai_message(message, \u001b[39m\"\u001b[39m\u001b[39massistant\u001b[39m\u001b[39m\"\u001b[39m, recipient)\n\u001b[1;32m    333\u001b[0m \u001b[39mif\u001b[39;00m valid:\n\u001b[0;32m--> 334\u001b[0m     recipient\u001b[39m.\u001b[39;49mreceive(message, \u001b[39mself\u001b[39;49m, request_reply, silent)\n\u001b[1;32m    335\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    336\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    337\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMessage can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    338\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:464\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    462\u001b[0m reply \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate_reply(messages\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchat_messages[sender], sender\u001b[39m=\u001b[39msender)\n\u001b[1;32m    463\u001b[0m \u001b[39mif\u001b[39;00m reply \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 464\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(reply, sender, silent\u001b[39m=\u001b[39;49msilent)\n",
      "File \u001b[0;32m~/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:334\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    332\u001b[0m valid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_append_oai_message(message, \u001b[39m\"\u001b[39m\u001b[39massistant\u001b[39m\u001b[39m\"\u001b[39m, recipient)\n\u001b[1;32m    333\u001b[0m \u001b[39mif\u001b[39;00m valid:\n\u001b[0;32m--> 334\u001b[0m     recipient\u001b[39m.\u001b[39;49mreceive(message, \u001b[39mself\u001b[39;49m, request_reply, silent)\n\u001b[1;32m    335\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    336\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    337\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMessage can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    338\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:464\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    462\u001b[0m reply \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate_reply(messages\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchat_messages[sender], sender\u001b[39m=\u001b[39msender)\n\u001b[1;32m    463\u001b[0m \u001b[39mif\u001b[39;00m reply \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 464\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(reply, sender, silent\u001b[39m=\u001b[39;49msilent)\n",
      "    \u001b[0;31m[... skipping similar frames: ConversableAgent.send at line 334 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:464\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    462\u001b[0m reply \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate_reply(messages\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchat_messages[sender], sender\u001b[39m=\u001b[39msender)\n\u001b[1;32m    463\u001b[0m \u001b[39mif\u001b[39;00m reply \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 464\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(reply, sender, silent\u001b[39m=\u001b[39;49msilent)\n",
      "File \u001b[0;32m~/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:334\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    332\u001b[0m valid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_append_oai_message(message, \u001b[39m\"\u001b[39m\u001b[39massistant\u001b[39m\u001b[39m\"\u001b[39m, recipient)\n\u001b[1;32m    333\u001b[0m \u001b[39mif\u001b[39;00m valid:\n\u001b[0;32m--> 334\u001b[0m     recipient\u001b[39m.\u001b[39;49mreceive(message, \u001b[39mself\u001b[39;49m, request_reply, silent)\n\u001b[1;32m    335\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    336\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    337\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMessage can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    338\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:462\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[39mif\u001b[39;00m request_reply \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mor\u001b[39;00m request_reply \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreply_at_receive[sender] \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 462\u001b[0m reply \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_reply(messages\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchat_messages[sender], sender\u001b[39m=\u001b[39;49msender)\n\u001b[1;32m    463\u001b[0m \u001b[39mif\u001b[39;00m reply \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(reply, sender, silent\u001b[39m=\u001b[39msilent)\n",
      "File \u001b[0;32m~/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:779\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[0;34m(self, messages, sender, exclude)\u001b[0m\n\u001b[1;32m    777\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    778\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[39m\"\u001b[39m\u001b[39mtrigger\u001b[39m\u001b[39m\"\u001b[39m], sender):\n\u001b[0;32m--> 779\u001b[0m     final, reply \u001b[39m=\u001b[39m reply_func(\u001b[39mself\u001b[39;49m, messages\u001b[39m=\u001b[39;49mmessages, sender\u001b[39m=\u001b[39;49msender, config\u001b[39m=\u001b[39;49mreply_func_tuple[\u001b[39m\"\u001b[39;49m\u001b[39mconfig\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    780\u001b[0m     \u001b[39mif\u001b[39;00m final:\n\u001b[1;32m    781\u001b[0m         \u001b[39mreturn\u001b[39;00m reply\n",
      "File \u001b[0;32m~/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:658\u001b[0m, in \u001b[0;36mConversableAgent.generate_function_call_reply\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m    656\u001b[0m message \u001b[39m=\u001b[39m messages[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    657\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mfunction_call\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m message:\n\u001b[0;32m--> 658\u001b[0m     _, func_return \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute_function(message[\u001b[39m\"\u001b[39;49m\u001b[39mfunction_call\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    659\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m, func_return\n\u001b[1;32m    660\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:990\u001b[0m, in \u001b[0;36mConversableAgent.execute_function\u001b[0;34m(self, func_call)\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[39mprint\u001b[39m(\n\u001b[1;32m    986\u001b[0m     colored(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m>>>>>>>> EXECUTING FUNCTION \u001b[39m\u001b[39m{\u001b[39;00mfunc_name\u001b[39m}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmagenta\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m    987\u001b[0m     flush\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    988\u001b[0m )\n\u001b[1;32m    989\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 990\u001b[0m     content \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49marguments)\n\u001b[1;32m    991\u001b[0m     is_exec_success \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    992\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;32m/Users/jarvixwang/Documents/Project/autogen/notebook/agentchat_two_users.ipynb 单元格 12\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jarvixwang/Documents/Project/autogen/notebook/agentchat_two_users.ipynb#X13sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m expert \u001b[39m=\u001b[39m autogen\u001b[39m.\u001b[39mUserProxyAgent(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jarvixwang/Documents/Project/autogen/notebook/agentchat_two_users.ipynb#X13sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mexpert\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jarvixwang/Documents/Project/autogen/notebook/agentchat_two_users.ipynb#X13sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     human_input_mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mALWAYS\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jarvixwang/Documents/Project/autogen/notebook/agentchat_two_users.ipynb#X13sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     max_consecutive_auto_reply\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jarvixwang/Documents/Project/autogen/notebook/agentchat_two_users.ipynb#X13sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     code_execution_config\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mwork_dir\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mexpert\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jarvixwang/Documents/Project/autogen/notebook/agentchat_two_users.ipynb#X13sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jarvixwang/Documents/Project/autogen/notebook/agentchat_two_users.ipynb#X13sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# 专家接收到了任务，将这个任务发送给他的助理\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jarvixwang/Documents/Project/autogen/notebook/agentchat_two_users.ipynb#X13sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m expert\u001b[39m.\u001b[39;49minitiate_chat(assistant_for_expert, message\u001b[39m=\u001b[39;49mmessage)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jarvixwang/Documents/Project/autogen/notebook/agentchat_two_users.ipynb#X13sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# TODO: stop_reply_at_receive没懂\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jarvixwang/Documents/Project/autogen/notebook/agentchat_two_users.ipynb#X13sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m expert\u001b[39m.\u001b[39mstop_reply_at_receive(assistant_for_expert)\n",
      "File \u001b[0;32m~/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:531\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[0;34m(self, recipient, clear_history, silent, **context)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Initiate a chat with the recipient agent.\u001b[39;00m\n\u001b[1;32m    518\u001b[0m \n\u001b[1;32m    519\u001b[0m \u001b[39mReset the consecutive auto reply counter.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[39m        \"message\" needs to be provided if the `generate_init_message` method is not overridden.\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    530\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_chat(recipient, clear_history)\n\u001b[0;32m--> 531\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_init_message(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcontext), recipient, silent\u001b[39m=\u001b[39;49msilent)\n",
      "File \u001b[0;32m~/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:334\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    332\u001b[0m valid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_append_oai_message(message, \u001b[39m\"\u001b[39m\u001b[39massistant\u001b[39m\u001b[39m\"\u001b[39m, recipient)\n\u001b[1;32m    333\u001b[0m \u001b[39mif\u001b[39;00m valid:\n\u001b[0;32m--> 334\u001b[0m     recipient\u001b[39m.\u001b[39;49mreceive(message, \u001b[39mself\u001b[39;49m, request_reply, silent)\n\u001b[1;32m    335\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    336\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    337\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMessage can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    338\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:464\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    462\u001b[0m reply \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate_reply(messages\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchat_messages[sender], sender\u001b[39m=\u001b[39msender)\n\u001b[1;32m    463\u001b[0m \u001b[39mif\u001b[39;00m reply \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 464\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(reply, sender, silent\u001b[39m=\u001b[39;49msilent)\n",
      "File \u001b[0;32m~/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:334\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    332\u001b[0m valid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_append_oai_message(message, \u001b[39m\"\u001b[39m\u001b[39massistant\u001b[39m\u001b[39m\"\u001b[39m, recipient)\n\u001b[1;32m    333\u001b[0m \u001b[39mif\u001b[39;00m valid:\n\u001b[0;32m--> 334\u001b[0m     recipient\u001b[39m.\u001b[39;49mreceive(message, \u001b[39mself\u001b[39;49m, request_reply, silent)\n\u001b[1;32m    335\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    336\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    337\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMessage can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    338\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:462\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[39mif\u001b[39;00m request_reply \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mor\u001b[39;00m request_reply \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreply_at_receive[sender] \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 462\u001b[0m reply \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_reply(messages\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchat_messages[sender], sender\u001b[39m=\u001b[39;49msender)\n\u001b[1;32m    463\u001b[0m \u001b[39mif\u001b[39;00m reply \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(reply, sender, silent\u001b[39m=\u001b[39msilent)\n",
      "File \u001b[0;32m~/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:779\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[0;34m(self, messages, sender, exclude)\u001b[0m\n\u001b[1;32m    777\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    778\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[39m\"\u001b[39m\u001b[39mtrigger\u001b[39m\u001b[39m\"\u001b[39m], sender):\n\u001b[0;32m--> 779\u001b[0m     final, reply \u001b[39m=\u001b[39m reply_func(\u001b[39mself\u001b[39;49m, messages\u001b[39m=\u001b[39;49mmessages, sender\u001b[39m=\u001b[39;49msender, config\u001b[39m=\u001b[39;49mreply_func_tuple[\u001b[39m\"\u001b[39;49m\u001b[39mconfig\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    780\u001b[0m     \u001b[39mif\u001b[39;00m final:\n\u001b[1;32m    781\u001b[0m         \u001b[39mreturn\u001b[39;00m reply\n",
      "File \u001b[0;32m~/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:677\u001b[0m, in \u001b[0;36mConversableAgent.check_termination_and_human_reply\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m    675\u001b[0m no_human_input_msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    676\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhuman_input_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mALWAYS\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 677\u001b[0m     reply \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_human_input(\n\u001b[1;32m    678\u001b[0m         \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mProvide feedback to \u001b[39;49m\u001b[39m{\u001b[39;49;00msender\u001b[39m.\u001b[39;49mname\u001b[39m}\u001b[39;49;00m\u001b[39m. Press enter to skip and use auto-reply, or type \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mexit\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m to end the conversation: \u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m    679\u001b[0m     )\n\u001b[1;32m    680\u001b[0m     no_human_input_msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mNO HUMAN INPUT RECEIVED.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m reply \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    681\u001b[0m     \u001b[39m# if the human input is empty, and the message is a termination message, then we will terminate the conversation\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:866\u001b[0m, in \u001b[0;36mConversableAgent.get_human_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    855\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_human_input\u001b[39m(\u001b[39mself\u001b[39m, prompt: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m    856\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get human input.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m \n\u001b[1;32m    858\u001b[0m \u001b[39m    Override this method to customize the way to get human input.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    864\u001b[0m \u001b[39m        str: human input.\u001b[39;00m\n\u001b[1;32m    865\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 866\u001b[0m     reply \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m(prompt)\n\u001b[1;32m    867\u001b[0m     \u001b[39mreturn\u001b[39;00m reply\n",
      "File \u001b[0;32m~/anaconda3/envs/autogen/lib/python3.11/site-packages/ipykernel/kernelbase.py:1202\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1200\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1201\u001b[0m     \u001b[39mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1202\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_input_request(\n\u001b[1;32m   1203\u001b[0m     \u001b[39mstr\u001b[39;49m(prompt),\n\u001b[1;32m   1204\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parent_ident[\u001b[39m\"\u001b[39;49m\u001b[39mshell\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m   1205\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_parent(\u001b[39m\"\u001b[39;49m\u001b[39mshell\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1206\u001b[0m     password\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1207\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/autogen/lib/python3.11/site-packages/ipykernel/kernelbase.py:1245\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1242\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1243\u001b[0m     \u001b[39m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1244\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mInterrupted by user\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1245\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1246\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   1247\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog\u001b[39m.\u001b[39mwarning(\u001b[39m\"\u001b[39m\u001b[39mInvalid Message:\u001b[39m\u001b[39m\"\u001b[39m, exc_info\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# the assistant receives a message from the student, which contains the task description\n",
    "student.initiate_chat(\n",
    "    assistant_for_student,\n",
    "    message=\"\"\"Find $a + b + c$, given that $x+y \\\\neq -1$ and \n",
    "\\\\begin{align}\n",
    "\tax + by + c & = x + 7,\\\\\n",
    "\ta + bx + cy & = 2x + 6y,\\\\\n",
    "\tay + b + cx & = 4x + y.\n",
    "\\\\end{align}.\n",
    "\"\"\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the assistant needs to consult the expert, it suggests a function call to `ask_expert`. When this happens, a line like the following will be displayed:\n",
    "\n",
    "***** Suggested function Call: ask_expert *****\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "2d910cfd2d2a4fc49fc30fbbdc5576a7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "454146d0f7224f038689031002906e6f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e4ae2b6f5a974fd4bafb6abb9d12ff26",
        "IPY_MODEL_577e1e3cc4db4942b0883577b3b52755",
        "IPY_MODEL_b40bdfb1ac1d4cffb7cefcb870c64d45"
       ],
       "layout": "IPY_MODEL_dc83c7bff2f241309537a8119dfc7555",
       "tabbable": null,
       "tooltip": null
      }
     },
     "577e1e3cc4db4942b0883577b3b52755": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2d910cfd2d2a4fc49fc30fbbdc5576a7",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_74a6ba0c3cbc4051be0a83e152fe1e62",
       "tabbable": null,
       "tooltip": null,
       "value": 1
      }
     },
     "6086462a12d54bafa59d3c4566f06cb2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "74a6ba0c3cbc4051be0a83e152fe1e62": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "7d3f3d9e15894d05a4d188ff4f466554": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b40bdfb1ac1d4cffb7cefcb870c64d45": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f1355871cc6f4dd4b50d9df5af20e5c8",
       "placeholder": "​",
       "style": "IPY_MODEL_ca245376fd9f4354af6b2befe4af4466",
       "tabbable": null,
       "tooltip": null,
       "value": " 1/1 [00:00&lt;00:00, 44.69it/s]"
      }
     },
     "ca245376fd9f4354af6b2befe4af4466": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "dc83c7bff2f241309537a8119dfc7555": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e4ae2b6f5a974fd4bafb6abb9d12ff26": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6086462a12d54bafa59d3c4566f06cb2",
       "placeholder": "​",
       "style": "IPY_MODEL_7d3f3d9e15894d05a4d188ff4f466554",
       "tabbable": null,
       "tooltip": null,
       "value": "100%"
      }
     },
     "f1355871cc6f4dd4b50d9df5af20e5c8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
